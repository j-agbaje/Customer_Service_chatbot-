{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jgdqfHoSJeAb"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive for saving results and models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7yg9GxZeI8J",
        "outputId": "425fa6bc-bf01-4cf8-8e11-b07d0fc79891"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD BEST MODEL\n",
        "print(\"Loading Model....\")\n",
        "MODEL_PATH = '/content/drive/MyDrive/chatbot_model_Higher_LR_Extended'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
        "print(\"Model loaded successfully!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiW4uu1NeEys",
        "outputId": "8b74bc6d-1b21-49bf-faea-c0cecfa22b11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/chatbot_model_Higher_LR_Extended.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # function for taking in input and displaying output\n",
        "def display_output(user_input, history):\n",
        "    # fall back message if message is not in domain\n",
        "    if not is_in_domain(message):\n",
        "        fallback = (\"I apologize, but I can only assist with customer service queries \"\n",
        "                   \"related to orders, refunds, shipping, accounts, and payments. \"\n",
        "                   \"Please contact our support team for other inquiries at 1-800-123-4567.\")\n",
        "        yield fallback\n",
        "        return\n",
        "\n",
        "    query = f\"Answer the customer service query:{user_input}\" # format query to fit model input\n",
        "\n",
        "\n",
        "    inputs = tokenizer(query, return_tensors='tf', padding=True, truncation=True) # tokenize inut\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 256) # generate output\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True) # decode output\n",
        "    for i in range(len(response)):\n",
        "        # time.sleep(0.1)\n",
        "        yield response[: i+1]\n",
        "\n"
      ],
      "metadata": {
        "id": "wy_UcqCC-hY3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# gradio chatbot  interface\n",
        "gr.ChatInterface(\n",
        "    fn=display_output,\n",
        "    type=\"messages\",\n",
        "    title=\"ðŸ’¬ Customer Support Assistant\",\n",
        "    description=(\n",
        "        \"Welcome to the **Customer Service Chatbot** powered by Google FLAN-T5 and the Bitext dataset.\\n\\n\"\n",
        "        \"You can ask questions related to customer support, such as:\\n\"\n",
        "        \"- Cancelling or tracking an order\\n\"\n",
        "        \"- Requesting a refund\\n\"\n",
        "        \"- Updating your account details and more FAQs\\n\\n\"\n",
        "        \"ðŸ’¡ *Type your question below and press Enter or click 'Submit' to get a response.*\"\n",
        "    ),\n",
        "    examples=[\n",
        "        [\"I want to cancel my order #12345\"],\n",
        "        [\"Can you help me track my shipment?\"],\n",
        "        [\"I'd like to request a refund for my last purchase.\"],\n",
        "        [\"How do I change my account password?\"],\n",
        "        [\"What's your return policy?\"],\n",
        "        [\"I want to change my shipping address for order 98765\"],\n",
        "    ],\n",
        "    textbox=gr.Textbox(\n",
        "        placeholder=\"Ask your question here... (e.g. 'How can I cancel my order?')\",\n",
        "        container=False,\n",
        "        autoscroll=True,\n",
        "        scale=7\n",
        "    ),\n",
        "    theme=gr.themes.Base(primary_hue=\"blue\").set(\n",
        "        body_background_fill=\"#0f172a\",  # Dark blue-gray - banking/finance vibe\n",
        "        body_background_fill_dark=\"#0f172a\",\n",
        "        body_text_color=\"#ffffff\",  # White text\n",
        "        body_text_color_dark=\"#ffffff\",\n",
        "        block_label_text_color=\"#ffffff\",\n",
        "        block_title_text_color=\"#ffffff\",\n",
        "        input_background_fill=\"*neutral_800\",\n",
        "        button_primary_text_color=\"#ffffff\",\n",
        "        color_accent=\"#06b6d4\",  # Cyan - calming, supportive\n",
        "        color_accent_soft=\"#06b6d4\",\n",
        "    ),\n",
        ").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "dpfXkwQR-10-",
        "outputId": "fef57ed8-c404-447e-d529-a0a5482507e9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6f7a5c593b0a6ec269.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6f7a5c593b0a6ec269.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}